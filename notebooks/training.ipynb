{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rasOe6jNJbFj",
        "colab_type": "text"
      },
      "source": [
        "# How to train a ResNet50 on RxRx1 using TPUs \n",
        "\n",
        "Colaboratory makes it easy to train models using [Cloud TPUs](https://cloud.google.com/tpu/), and this notebook demonstrates how to use the code in [rxrx1-utils](https://github.com/recursionpharma/rxrx1-utils) to train ResNet50 on the RxRx1 image set using Colab TPU.\n",
        "\n",
        "Be sure to select the TPU runtime before beginning!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKtZctcXJTAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNgr17uD0K--",
        "colab_type": "code",
        "outputId": "dc2b6d1b-fa7d-481d-c5ef-887876f6c27a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "    !git clone https://github.com/recursionpharma/rxrx1-utils\n",
        "    sys.path.append('/content/rxrx1-utils')\n",
        "\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    \n",
        "from rxrx.main import main"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rxrx1-utils'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 99 (delta 48), reused 92 (delta 42), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (99/99), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HrPeVFofzIdy"
      },
      "source": [
        "## Train\n",
        "\n",
        "Set `MODEL_DIR` to be a Google Cloud Storage bucket that you can write to.   The code will write your checkpoins to this directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9MjRJpwJTAw",
        "colab_type": "code",
        "outputId": "44655cb7-37c9-4a20-a689-6cda882224ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "MODEL_DIR = 'gs://path/to/your/bucket'\n",
        "URL_BASE_PATH = 'gs://rxrx1-us-central1/tfrecords/random-42'\n",
        "\n",
        "# make sure we're in a TPU runtime\n",
        "assert 'COLAB_TPU_ADDR' in os.environ\n",
        "\n",
        "# set TPU-relevant args\n",
        "tpu_grpc = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "num_shards = 8  # colab uses Cloud TPU v2-8\n",
        "\n",
        "# upload credentials to the TPU\n",
        "with tf.Session(tpu_grpc) as sess:\n",
        "    data = json.load(open('/content/adc.json'))\n",
        "    tf.contrib.cloud.configure_gcs(sess, credentials=data)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "main(use_tpu=True,\n",
        "     tpu=tpu_grpc,\n",
        "     gcp_project=None,\n",
        "     tpu_zone=None,\n",
        "     url_base_path=URL_BASE_PATH,\n",
        "     use_cache=False,\n",
        "     model_dir=MODEL_DIR,\n",
        "     train_epochs=1,\n",
        "     train_batch_size=512,\n",
        "     num_train_images=73030,\n",
        "     epochs_per_loop=1,\n",
        "     log_step_count_epochs=1,\n",
        "     num_cores=num_shards,\n",
        "     data_format='channels_last',\n",
        "     transpose_input=True,\n",
        "     tf_precision='bfloat16',\n",
        "     n_classes=1108,\n",
        "     momentum=0.9,\n",
        "     weight_decay=1e-4,\n",
        "     base_learning_rate=0.2,\n",
        "     warmup_epochs=5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 19:53:08.003671 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:280: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0627 19:53:08.005592 139758653511552 main.py:280] tpu: grpc://10.106.194.154:8470\n",
            "I0627 19:53:08.010348 139758653511552 main.py:283] gcp_project: None\n",
            "W0627 19:53:10.223041 139758653511552 estimator.py:1984] Estimator's model_fn (functools.partial(<function resnet_model_fn at 0x7f1be104d8c8>, n_classes=1108, num_train_images=73030, data_format='channels_last', transpose_input=True, train_batch_size=512, iterations_per_loop=142, tf_precision='bfloat16', momentum=0.9, weight_decay=0.0001, base_learning_rate=0.2, warmup_epochs=5, model_dir='gs://recursion-tpu-training/berton/rxrx1_test/my_test', use_tpu=True, resnet_depth=50)) includes params argument, but params are not passed to Estimator.\n",
            "I0627 19:53:10.225781 139758653511552 estimator.py:209] Using config: {'_model_dir': 'gs://recursion-tpu-training/berton/rxrx1_test/my_test', '_tf_random_seed': None, '_save_summary_steps': 142, '_save_checkpoints_steps': 142, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.106.194.154:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1bd4221eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.106.194.154:8470', '_evaluation_master': 'grpc://10.106.194.154:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=142, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f1bd4221240>}\n",
            "I0627 19:53:10.227086 139758653511552 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0627 19:53:10.233348 139758653511552 main.py:338] Train glob: gs://rxrx1-us-central1/tfrecords/random-42/train/*.tfrecord\n",
            "I0627 19:53:10.236295 139758653511552 main.py:351] Training for 142 steps (1.00 epochs in total). Current step 0.\n",
            "I0627 19:53:10.345630 139758653511552 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.106.194.154:8470) for TPU system metadata.\n",
            "I0627 19:53:10.362607 139758653511552 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0627 19:53:10.364023 139758653511552 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0627 19:53:10.365448 139758653511552 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0627 19:53:10.370308 139758653511552 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0627 19:53:10.373871 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9785620760386089044)\n",
            "I0627 19:53:10.379615 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 17957700996873846002)\n",
            "I0627 19:53:10.387336 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7534058317997506011)\n",
            "I0627 19:53:10.388670 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 732442551779127628)\n",
            "I0627 19:53:10.390994 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1258150734284970345)\n",
            "I0627 19:53:10.399887 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12528303070827221666)\n",
            "I0627 19:53:10.402188 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2385972351757131582)\n",
            "I0627 19:53:10.403303 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17540351673832642764)\n",
            "I0627 19:53:10.406220 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16943942441228138344)\n",
            "I0627 19:53:10.408085 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14676506593360444113)\n",
            "I0627 19:53:10.410236 139758653511552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10007324885980858573)\n",
            "W0627 19:53:10.435066 139758653511552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0627 19:53:10.457272 139758653511552 estimator.py:1145] Calling model_fn.\n",
            "W0627 19:53:10.489276 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/input.py:94: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
            "W0627 19:53:10.490580 139758653511552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/shuffle_ops.py:54: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0627 19:53:10.501322 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/input.py:115: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0627 19:53:10.502724 139758653511552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0627 19:53:10.540040 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/input.py:125: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0627 19:53:10.541434 139758653511552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0627 19:53:10.547796 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/input.py:48: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0627 19:53:10.550729 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/input.py:59: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0627 19:53:10.683037 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/official_resnet.py:211: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0627 19:53:10.946627 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/official_resnet.py:70: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W0627 19:53:11.076068 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/official_resnet.py:413: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "W0627 19:53:15.471554 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/official_resnet.py:442: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.AveragePooling2D instead.\n",
            "W0627 19:53:15.481713 139758653511552 deprecation.py:323] From /content/rxrx1-utils/rxrx/official_resnet.py:449: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0627 19:53:16.032349 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:125: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n",
            "\n",
            "W0627 19:53:16.092159 139758653511552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 19:53:16.112515 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:131: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0627 19:53:16.223887 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:138: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
            "\n",
            "W0627 19:53:16.239456 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:145: The name tf.train.cosine_decay_restarts is deprecated. Please use tf.compat.v1.train.cosine_decay_restarts instead.\n",
            "\n",
            "W0627 19:53:16.297146 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:155: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "W0627 19:53:16.298741 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:167: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0627 19:53:16.300213 139758653511552 deprecation_wrapper.py:119] From /content/rxrx1-utils/rxrx/main.py:167: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "I0627 19:53:22.770973 139758653511552 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0627 19:53:23.122148 139758653511552 estimator.py:1147] Done calling model_fn.\n",
            "I0627 19:53:25.609339 139758653511552 tpu_estimator.py:499] TPU job name worker\n",
            "I0627 19:53:27.020166 139758653511552 monitored_session.py:240] Graph was finalized.\n",
            "I0627 19:53:31.370273 139758653511552 session_manager.py:500] Running local_init_op.\n",
            "I0627 19:53:31.856439 139758653511552 session_manager.py:502] Done running local_init_op.\n",
            "I0627 19:53:39.210234 139758653511552 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://recursion-tpu-training/berton/rxrx1_test/my_test/model.ckpt.\n",
            "W0627 19:53:47.069951 139758653511552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0627 19:53:48.202275 139758653511552 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0627 19:53:48.204611 139758653511552 session_support.py:332] Installing graceful shutdown hook.\n",
            "I0627 19:53:48.214668 139758653511552 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0627 19:53:48.222669 139758653511552 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0627 19:53:48.228841 139758653511552 tpu_estimator.py:557] Init TPU system\n",
            "I0627 19:53:52.087585 139758653511552 tpu_estimator.py:566] Initialized TPU in 3 seconds\n",
            "I0627 19:53:52.887957 139757323314944 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0627 19:53:52.888870 139757314922240 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0627 19:53:53.404005 139758653511552 tpu_estimator.py:590] Enqueue next (142) batch(es) of data to infeed.\n",
            "I0627 19:53:53.406334 139758653511552 tpu_estimator.py:594] Dequeue next (142) batch(es) of data from outfeed.\n",
            "I0627 19:54:33.920991 139757314922240 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0627 19:55:34.432938 139757314922240 tpu_estimator.py:275] Outfeed finished for iteration (0, 80)\n",
            "I0627 19:56:20.688752 139758653511552 basic_session_run_hooks.py:606] Saving checkpoints for 142 into gs://recursion-tpu-training/berton/rxrx1_test/my_test/model.ckpt.\n",
            "I0627 19:56:27.875086 139758653511552 basic_session_run_hooks.py:262] loss = 7.9618587, step = 142\n",
            "I0627 19:56:28.358581 139758653511552 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0627 19:56:28.359947 139758653511552 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0627 19:56:28.364674 139757323314944 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0627 19:56:28.366270 139757323314944 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0627 19:56:28.369237 139758653511552 error_handling.py:96] infeed marked as finished\n",
            "I0627 19:56:28.371149 139758653511552 tpu_estimator.py:602] Stop output thread controller\n",
            "I0627 19:56:28.373593 139758653511552 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0627 19:56:28.377429 139757314922240 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0627 19:56:28.379346 139757314922240 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0627 19:56:28.381241 139758653511552 error_handling.py:96] outfeed marked as finished\n",
            "I0627 19:56:28.382400 139758653511552 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0627 19:56:32.223384 139758653511552 estimator.py:368] Loss for final step: 7.9618587.\n",
            "I0627 19:56:32.225899 139758653511552 error_handling.py:96] training_loop marked as finished\n",
            "I0627 19:56:32.231190 139758653511552 main.py:358] Finished training up to step 142. Elapsed seconds 201.\n",
            "I0627 19:56:32.232772 139758653511552 main.py:363] Finished training up to step 142. Elapsed seconds 201.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
